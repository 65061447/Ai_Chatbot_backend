import pandas as pd
import faiss
import numpy as np
import pickle
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import CharacterTextSplitter

# 1. Load CSV
df = pd.read_csv("allattractions_with_context.csv")
print("‚úÖ CSV loaded with shape:", df.shape)

# 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ column 'context' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
if "context" not in df.columns:
    raise ValueError("‚ùå The CSV must have a 'context' column")

# 3. ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'context'
context_list = df["context"].dropna().astype(str).tolist()
print(f"‚úÖ Loaded {len(context_list)} non-empty context rows")

all_text = "\n".join(context_list)
print("üìù Sample context text (first 500 chars):\n", all_text[:500])

# 4. ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô chunks
splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=100
)
chunks = splitter.split_text(all_text)
print(f"üß© Text split into {len(chunks)} chunks")
print("üìé Example chunk:\n", chunks[0][:300])

# 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å chunks ‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• BAAI/bge-m3
print("üîÑ Encoding chunks with model 'BAAI/bge-m3'...")
model = SentenceTransformer("BAAI/bge-m3")
embeddings = model.encode(chunks, show_progress_bar=True)
print("‚úÖ Embedding shape:", np.array(embeddings).shape)

# 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS index ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° embeddings ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
index = faiss.IndexFlatL2(embeddings[0].shape[0])
index.add(np.array(embeddings))
print("üì¶ FAISS index created and populated")

# 7. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å index ‡πÅ‡∏•‡∏∞ chunks
faiss.write_index(index, "vector_index.faiss")
with open("chunks.pkl", "wb") as f:
    pickle.dump(chunks, f)

print(f"\n‚úÖ Vector store created successfully with {len(chunks)} chunks")
